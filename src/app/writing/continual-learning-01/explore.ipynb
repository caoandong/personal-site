{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2187d6b2",
   "metadata": {},
   "source": [
    "The Standard Benchmark: Split CIFAR-10\n",
    "\n",
    "This is the \"Hello World\" of modern continual learning.\n",
    "\n",
    "The Split: The 10 classes of CIFAR-10 are divided into 5 discrete tasks.\n",
    "\n",
    "Classes per Task: Each task introduces 2 new classes (e.g., Task 1: Airplane/Auto, Task 2: Bird/Cat, etc.).\n",
    "\n",
    "The Goal: The model trains on Task 1, then Task 2, and so on. After training on Task 5, it is evaluated on the test sets of all 10 classes simultaneously.\n",
    "\n",
    "Class-Incremental Learning (CIL)\n",
    "\n",
    "Difficulty: Hard (Standard for modern research).\n",
    "\n",
    "Setup: The model does not know which task an image belongs to at inference time. It must choose from all 10 possible classes (single-head evaluation).\n",
    "\n",
    "Why choose this: This is the most realistic setting. If you are reading papers from 2021â€“2024, they likely use this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379387e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "from math import ceil\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "CIFAR_MEAN = torch.tensor((0.4914, 0.4822, 0.4465))\n",
    "CIFAR_STD = torch.tensor((0.2470, 0.2435, 0.2616))\n",
    "\n",
    "def batch_flip_lr(inputs):\n",
    "    flip_mask = (torch.rand(len(inputs), device=inputs.device) < 0.5).view(-1, 1, 1, 1)\n",
    "    return torch.where(flip_mask, inputs.flip(-1), inputs)\n",
    "\n",
    "def batch_crop(images, crop_size):\n",
    "    r = (images.size(-1) - crop_size)//2\n",
    "    shifts = torch.randint(-r, r+1, size=(len(images), 2), device=images.device)\n",
    "    images_out = torch.empty((len(images), 3, crop_size, crop_size), device=images.device, dtype=images.dtype)\n",
    "    # The two cropping methods in this if-else produce equivalent results, but the second is faster for r > 2.\n",
    "    if r <= 2:\n",
    "        for sy in range(-r, r+1):\n",
    "            for sx in range(-r, r+1):\n",
    "                mask = (shifts[:, 0] == sy) & (shifts[:, 1] == sx)\n",
    "                images_out[mask] = images[mask, :, r+sy:r+sy+crop_size, r+sx:r+sx+crop_size]\n",
    "    else:\n",
    "        images_tmp = torch.empty((len(images), 3, crop_size, crop_size+2*r), device=images.device, dtype=images.dtype)\n",
    "        for s in range(-r, r+1):\n",
    "            mask = (shifts[:, 0] == s)\n",
    "            images_tmp[mask] = images[mask, :, r+s:r+s+crop_size, :]\n",
    "        for s in range(-r, r+1):\n",
    "            mask = (shifts[:, 1] == s)\n",
    "            images_out[mask] = images_tmp[mask, :, :, r+s:r+s+crop_size]\n",
    "    return images_out\n",
    "\n",
    "class CifarLoader:\n",
    "\n",
    "    def __init__(self, path, train=True, batch_size=500, aug=None):\n",
    "        data_path = os.path.join(path, \"train.pt\" if train else \"test.pt\")\n",
    "        if not os.path.exists(data_path):\n",
    "            dset = torchvision.datasets.CIFAR10(path, download=True, train=train)\n",
    "            images = torch.tensor(dset.data)\n",
    "            labels = torch.tensor(dset.targets)\n",
    "            torch.save({\"images\": images, \"labels\": labels, \"classes\": dset.classes}, data_path)\n",
    "\n",
    "        data = torch.load(data_path, map_location=torch.device(\"cuda\"))\n",
    "        self.images, self.labels, self.classes = data[\"images\"], data[\"labels\"], data[\"classes\"]\n",
    "        # It's faster to load+process uint8 data than to load preprocessed fp16 data\n",
    "        self.images = (self.images.half() / 255).permute(0, 3, 1, 2).to(memory_format=torch.channels_last)\n",
    "\n",
    "        self.normalize = T.Normalize(CIFAR_MEAN, CIFAR_STD)\n",
    "        self.proc_images = {} # Saved results of image processing to be done on the first epoch\n",
    "        self.epoch = 0\n",
    "\n",
    "        self.aug = aug or {}\n",
    "        for k in self.aug.keys():\n",
    "            assert k in [\"flip\", \"translate\"], \"Unrecognized key: %s\" % k\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = train\n",
    "        self.shuffle = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)//self.batch_size if self.drop_last else ceil(len(self.images)/self.batch_size)\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        if self.epoch == 0:\n",
    "            images = self.proc_images[\"norm\"] = self.normalize(self.images)\n",
    "            # Pre-flip images in order to do every-other epoch flipping scheme\n",
    "            if self.aug.get(\"flip\", False):\n",
    "                images = self.proc_images[\"flip\"] = batch_flip_lr(images)\n",
    "            # Pre-pad images to save time when doing random translation\n",
    "            pad = self.aug.get(\"translate\", 0)\n",
    "            if pad > 0:\n",
    "                self.proc_images[\"pad\"] = F.pad(images, (pad,)*4, \"reflect\")\n",
    "\n",
    "        if self.aug.get(\"translate\", 0) > 0:\n",
    "            images = batch_crop(self.proc_images[\"pad\"], self.images.shape[-2])\n",
    "        elif self.aug.get(\"flip\", False):\n",
    "            images = self.proc_images[\"flip\"]\n",
    "        else:\n",
    "            images = self.proc_images[\"norm\"]\n",
    "        # Flip all images together every other epoch. This increases diversity relative to random flipping\n",
    "        if self.aug.get(\"flip\", False):\n",
    "            if self.epoch % 2 == 1:\n",
    "                images = images.flip(-1)\n",
    "\n",
    "        self.epoch += 1\n",
    "\n",
    "        indices = (torch.randperm if self.shuffle else torch.arange)(len(images), device=images.device)\n",
    "        for i in range(len(self)):\n",
    "            idxs = indices[i*self.batch_size:(i+1)*self.batch_size]\n",
    "            yield (images[idxs], self.labels[idxs])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal-site",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
