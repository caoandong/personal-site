{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2099291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Small example for visualization\n",
    "sequence_length = 8\n",
    "head_dimension = 4\n",
    "\n",
    "np.random.seed(42)\n",
    "query = np.random.randn(sequence_length, head_dimension).astype(np.float32)\n",
    "key = np.random.randn(sequence_length, head_dimension).astype(np.float32)\n",
    "value = np.random.randn(sequence_length, head_dimension).astype(np.float32)\n",
    "\n",
    "print(f\"query shape: {query.shape}\")\n",
    "print(f\"key shape: {key.shape}\")\n",
    "print(f\"value shape: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2123f354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# NAIVE ATTENTION - materializes the full (sequence_length x sequence_length) matrix\n",
    "# =============================================================================\n",
    "\n",
    "def naive_attention(query, key, value):\n",
    "    \"\"\"\n",
    "    Standard attention: O = softmax(Q @ K^T / sqrt(d)) @ V\n",
    "    \n",
    "    This materializes the full (N x N) attention matrix - what FlashAttention avoids.\n",
    "    \"\"\"\n",
    "    scale = 1.0 / math.sqrt(query.shape[-1])\n",
    "    \n",
    "    # Step 1: Compute all scores at once -> (sequence_length x sequence_length) matrix\n",
    "    scores = query @ key.T * scale\n",
    "    \n",
    "    # Step 2: Stable softmax (subtract max for numerical stability)\n",
    "    scores_max = scores.max(axis=1, keepdims=True)\n",
    "    exp_scores = np.exp(scores - scores_max)\n",
    "    attention_weights = exp_scores / exp_scores.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    # Step 3: Weighted sum of values\n",
    "    output = attention_weights @ value\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "naive_output, attention_weights = naive_attention(query, key, value)\n",
    "print(\"Naive attention output shape:\", naive_output.shape)\n",
    "print(\"\\nAttention weights (the N×N matrix we want to avoid storing):\")\n",
    "print(attention_weights.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ul2b592n40o",
   "metadata": {},
   "source": [
    "## The Online Softmax Trick\n",
    "\n",
    "The key insight: we can compute softmax **incrementally** without seeing all scores at once.\n",
    "\n",
    "**Problem**: For softmax, we need `exp(score_i) / sum(exp(all_scores))`. But we're processing scores in blocks!\n",
    "\n",
    "**Solution**: Maintain two running statistics per query row:\n",
    "- `running_max`: the maximum score seen so far\n",
    "- `running_sum_exp`: the sum of `exp(score - running_max)` for all scores seen\n",
    "\n",
    "When we see a new block of scores, we can **merge** the statistics:\n",
    "\n",
    "```\n",
    "new_max = max(running_max, block_max)\n",
    "new_sum_exp = exp(running_max - new_max) * running_sum_exp + exp(block_max - new_max) * block_sum_exp\n",
    "```\n",
    "\n",
    "The output also needs rescaling when the max changes - this is the \"correction factor\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a1iqfu42n6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth softmax: [0.0276 0.2037 0.0749 0.5536 0.0167 0.1235]\n",
      "After block [1. 3.]: max=3.00, sum_exp=1.1353\n",
      "After block [2. 4.]: max=4.00, sum_exp=1.5530\n",
      "After block [0.5 2.5]: max=4.00, sum_exp=1.8063\n",
      "\n",
      "Online softmax:       [0.0276 0.2037 0.0749 0.5536 0.0167 0.1235]\n",
      "Match: True\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DEMO: Online softmax on a single row of scores\n",
    "# =============================================================================\n",
    "# Let's prove the online softmax trick works before using it in flash attention\n",
    "\n",
    "scores_row = np.array([1.0, 3.0, 2.0, 4.0, 0.5, 2.5])  # pretend these are Q[i] @ K^T\n",
    "block_size = 2\n",
    "\n",
    "# Ground truth: compute softmax the normal way\n",
    "scores_max = scores_row.max()\n",
    "ground_truth = np.exp(scores_row - scores_max) / np.exp(scores_row - scores_max).sum()\n",
    "print(\"Ground truth softmax:\", ground_truth.round(4))\n",
    "\n",
    "# Online computation: process in blocks of 2\n",
    "running_max = -np.inf\n",
    "running_sum_exp = 0.0\n",
    "\n",
    "for block_start in range(0, len(scores_row), block_size):\n",
    "    block_scores = scores_row[block_start : block_start + block_size]\n",
    "    \n",
    "    # Compute block statistics\n",
    "    block_max = block_scores.max()\n",
    "    block_sum_exp = np.exp(block_scores - block_max).sum()\n",
    "    \n",
    "    # Merge with running statistics\n",
    "    new_max = max(running_max, block_max)\n",
    "    \n",
    "    # Key insight: rescale both old and new sums to the new max\n",
    "    old_contribution = np.exp(running_max - new_max) * running_sum_exp\n",
    "    new_contribution = np.exp(block_max - new_max) * block_sum_exp\n",
    "    new_sum_exp = old_contribution + new_contribution\n",
    "    \n",
    "    running_max = new_max\n",
    "    running_sum_exp = new_sum_exp\n",
    "    \n",
    "    print(f\"After block {block_scores}: max={running_max:.2f}, sum_exp={running_sum_exp:.4f}\")\n",
    "\n",
    "# Now we can compute the final softmax\n",
    "online_softmax = np.exp(scores_row - running_max) / running_sum_exp\n",
    "print(\"\\nOnline softmax:      \", online_softmax.round(4))\n",
    "print(\"Match:\", np.allclose(ground_truth, online_softmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frrikzaeilk",
   "metadata": {},
   "source": [
    "## Flash Attention: Combining Tiling + Online Softmax\n",
    "\n",
    "Now the tricky part: we need to maintain the **running output** too, not just softmax statistics.\n",
    "\n",
    "For each query row, we maintain:\n",
    "- `running_max`: maximum score seen so far  \n",
    "- `running_sum_exp`: sum of exp(scores - running_max)\n",
    "- `running_output`: the weighted sum, but we need to **rescale it** when max changes!\n",
    "\n",
    "The update formula when processing a new block:\n",
    "```\n",
    "new_max = max(running_max, block_max)\n",
    "\n",
    "# Correction factors for changing the \"coordinate system\"\n",
    "old_scale = exp(running_max - new_max)  \n",
    "new_scale = exp(block_max - new_max)\n",
    "\n",
    "new_sum_exp = old_scale * running_sum_exp + new_scale * block_sum_exp\n",
    "\n",
    "# The output update - this is the key insight!\n",
    "new_output = (old_scale * running_sum_exp * running_output + new_scale * block_weighted_values) / new_sum_exp\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cskrxzk4m6r",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FLASH ATTENTION - never materializes the full (N x N) matrix\n",
    "# =============================================================================\n",
    "\n",
    "def flash_attention(query, key, value, block_size_query=2, block_size_key=2):\n",
    "    \"\"\"\n",
    "    Flash Attention: computes exact attention without storing the full attention matrix.\n",
    "    \n",
    "    Key ideas:\n",
    "    1. Process query and key/value in small blocks (tiles)\n",
    "    2. Use online softmax to combine results from different blocks correctly\n",
    "    \n",
    "    Args:\n",
    "        query: (sequence_length, head_dimension)\n",
    "        key: (sequence_length, head_dimension)  \n",
    "        value: (sequence_length, head_dimension)\n",
    "        block_size_query: number of query rows to process together\n",
    "        block_size_key: number of key rows to process together\n",
    "    \"\"\"\n",
    "    sequence_length, head_dimension = query.shape\n",
    "    scale = 1.0 / math.sqrt(head_dimension)\n",
    "    \n",
    "    # Initialize output and running statistics for ALL query rows\n",
    "    output = np.zeros((sequence_length, head_dimension), dtype=np.float64)\n",
    "    running_max = np.full(sequence_length, -np.inf, dtype=np.float64)\n",
    "    running_sum_exp = np.zeros(sequence_length, dtype=np.float64)\n",
    "    \n",
    "    # Outer loop: iterate over blocks of keys/values\n",
    "    for key_block_start in range(0, sequence_length, block_size_key):\n",
    "        key_block_end = min(key_block_start + block_size_key, sequence_length)\n",
    "        \n",
    "        # Load one block of keys and values (in real GPU: load to SRAM)\n",
    "        key_block = key[key_block_start:key_block_end].astype(np.float64)\n",
    "        value_block = value[key_block_start:key_block_end].astype(np.float64)\n",
    "        \n",
    "        # Inner loop: iterate over blocks of queries\n",
    "        for query_block_start in range(0, sequence_length, block_size_query):\n",
    "            query_block_end = min(query_block_start + block_size_query, sequence_length)\n",
    "            \n",
    "            # Load one block of queries\n",
    "            query_block = query[query_block_start:query_block_end].astype(np.float64)\n",
    "            \n",
    "            # Get current running statistics for this query block\n",
    "            current_max = running_max[query_block_start:query_block_end]\n",
    "            current_sum_exp = running_sum_exp[query_block_start:query_block_end]\n",
    "            current_output = output[query_block_start:query_block_end]\n",
    "            \n",
    "            # =================================================================\n",
    "            # STEP 1: Compute scores for this (query_block x key_block) tile\n",
    "            # =================================================================\n",
    "            # This is the ONLY score matrix we compute - size (block_size_query x block_size_key)\n",
    "            # NOT (sequence_length x sequence_length)!\n",
    "            scores_block = (query_block @ key_block.T) * scale\n",
    "            \n",
    "            # =================================================================\n",
    "            # STEP 2: Compute LOCAL softmax statistics for this block\n",
    "            # =================================================================\n",
    "            block_max = scores_block.max(axis=1)  # max over keys, shape: (block_size_query,)\n",
    "            block_exp_scores = np.exp(scores_block - block_max[:, None])\n",
    "            block_sum_exp = block_exp_scores.sum(axis=1)  # shape: (block_size_query,)\n",
    "            \n",
    "            # Local weighted sum of values (not yet normalized)\n",
    "            block_weighted_values = block_exp_scores @ value_block  # shape: (block_size_query, head_dimension)\n",
    "            \n",
    "            # =================================================================\n",
    "            # STEP 3: MERGE with running statistics (the online softmax trick!)\n",
    "            # =================================================================\n",
    "            new_max = np.maximum(current_max, block_max)\n",
    "            \n",
    "            # Correction factors: rescale everything to the new maximum\n",
    "            old_scale = np.exp(current_max - new_max)  # how much to scale old statistics\n",
    "            new_scale = np.exp(block_max - new_max)    # how much to scale new block\n",
    "            \n",
    "            new_sum_exp = old_scale * current_sum_exp + new_scale * block_sum_exp\n",
    "            \n",
    "            # Update output: rescale old output and add new contribution, then normalize\n",
    "            # old_scale * current_sum_exp * current_output = old unnormalized weighted sum (rescaled)\n",
    "            # new_scale * block_weighted_values = new unnormalized weighted sum (rescaled)\n",
    "            new_output = (\n",
    "                old_scale[:, None] * current_sum_exp[:, None] * current_output +\n",
    "                new_scale[:, None] * block_weighted_values\n",
    "            ) / new_sum_exp[:, None]\n",
    "            \n",
    "            # =================================================================\n",
    "            # STEP 4: Write back updated statistics\n",
    "            # =================================================================\n",
    "            output[query_block_start:query_block_end] = new_output\n",
    "            running_max[query_block_start:query_block_end] = new_max\n",
    "            running_sum_exp[query_block_start:query_block_end] = new_sum_exp\n",
    "    \n",
    "    return output.astype(query.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ato3eubg8yf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive output:\n",
      "[[-0.2606  0.1387  0.187   0.178 ]\n",
      " [-0.2657  0.1958  0.3021  0.1886]\n",
      " [-0.2045  0.1917 -0.1595 -0.1489]\n",
      " [ 0.129  -0.2446  0.2879  0.6396]\n",
      " [-0.2408  0.0981 -0.2086 -0.4503]\n",
      " [ 0.058   0.6045 -0.2326  0.2756]\n",
      " [-0.1434 -0.0073 -0.0186 -0.1872]\n",
      " [-0.1516 -0.0692  0.1787 -0.2666]]\n",
      "\n",
      "Flash output:\n",
      "[[-0.2606  0.1387  0.187   0.178 ]\n",
      " [-0.2657  0.1958  0.3021  0.1886]\n",
      " [-0.2045  0.1917 -0.1595 -0.1489]\n",
      " [ 0.129  -0.2446  0.2879  0.6396]\n",
      " [-0.2408  0.0981 -0.2086 -0.4503]\n",
      " [ 0.058   0.6045 -0.2326  0.2756]\n",
      " [-0.1434 -0.0073 -0.0186 -0.1872]\n",
      " [-0.1516 -0.0692  0.1787 -0.2666]]\n",
      "\n",
      "Max absolute error: 5.9604645e-08\n",
      "Results match: True\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# COMPARISON: Verify flash attention matches naive attention\n",
    "# =============================================================================\n",
    "\n",
    "flash_output = flash_attention(query, key, value, block_size_query=2, block_size_key=2)\n",
    "\n",
    "print(\"Naive output:\")\n",
    "print(naive_output.round(4))\n",
    "print(\"\\nFlash output:\")\n",
    "print(flash_output.round(4))\n",
    "print(\"\\nMax absolute error:\", np.abs(naive_output - flash_output).max())\n",
    "print(\"Results match:\", np.allclose(naive_output, flash_output, rtol=1e-5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6uf78zofho",
   "metadata": {},
   "source": [
    "## Memory Analysis\n",
    "\n",
    "**Naive attention** stores:\n",
    "- Full score matrix: `(sequence_length × sequence_length)` \n",
    "- Full attention weights: `(sequence_length × sequence_length)`\n",
    "\n",
    "**Flash attention** stores:\n",
    "- One block of scores: `(block_size_query × block_size_key)` \n",
    "- Running statistics: `O(sequence_length)` for max and sum_exp\n",
    "\n",
    "For sequence_length=4096 with block_size=64:\n",
    "- Naive: 4096² × 2 = **33 million** floats\n",
    "- Flash: 64² + 4096×2 = **~12 thousand** floats\n",
    "\n",
    "That's a **2700x** memory reduction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qtd8ud1fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Test with larger dimensions and various block sizes\n",
    "# =============================================================================\n",
    "\n",
    "np.random.seed(123)\n",
    "large_sequence_length = 128\n",
    "large_head_dimension = 64\n",
    "\n",
    "large_query = np.random.randn(large_sequence_length, large_head_dimension).astype(np.float32)\n",
    "large_key = np.random.randn(large_sequence_length, large_head_dimension).astype(np.float32)\n",
    "large_value = np.random.randn(large_sequence_length, large_head_dimension).astype(np.float32)\n",
    "\n",
    "large_naive_output, _ = naive_attention(large_query, large_key, large_value)\n",
    "\n",
    "# Test various block sizes\n",
    "for block_size in [8, 16, 32, 64]:\n",
    "    large_flash_output = flash_attention(\n",
    "        large_query, large_key, large_value,\n",
    "        block_size_query=block_size, \n",
    "        block_size_key=block_size\n",
    "    )\n",
    "    max_error = np.abs(large_naive_output - large_flash_output).max()\n",
    "    print(f\"Block size {block_size:2d}: max error = {max_error:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bla6leoke5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query shape: (8, 4)\n",
      "key shape: (8, 4)\n",
      "value shape: (8, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Small example for visualization\n",
    "sequence_length = 8\n",
    "head_dimension = 4\n",
    "\n",
    "np.random.seed(42)\n",
    "query = np.random.randn(sequence_length, head_dimension).astype(np.float32)\n",
    "key = np.random.randn(sequence_length, head_dimension).astype(np.float32)\n",
    "value = np.random.randn(sequence_length, head_dimension).astype(np.float32)\n",
    "\n",
    "print(f\"query shape: {query.shape}\")\n",
    "print(f\"key shape: {key.shape}\")\n",
    "print(f\"value shape: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6p08f1sn0us",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive attention output shape: (8, 4)\n",
      "\n",
      "Attention weights (the N×N matrix we want to avoid storing):\n",
      "[[0.059 0.098 0.098 0.151 0.11  0.248 0.209 0.027]\n",
      " [0.124 0.042 0.067 0.124 0.121 0.298 0.198 0.026]\n",
      " [0.108 0.095 0.127 0.133 0.076 0.115 0.108 0.238]\n",
      " [0.055 0.577 0.032 0.053 0.138 0.005 0.02  0.119]\n",
      " [0.125 0.095 0.083 0.1   0.065 0.045 0.057 0.43 ]\n",
      " [0.261 0.115 0.198 0.016 0.199 0.029 0.027 0.155]\n",
      " [0.048 0.192 0.085 0.232 0.065 0.083 0.124 0.173]\n",
      " [0.02  0.152 0.042 0.361 0.05  0.133 0.205 0.037]]\n"
     ]
    }
   ],
   "source": [
    "# NAIVE ATTENTION\n",
    "def naive_attention(query, key, value):\n",
    "    scale = 1.0 / math.sqrt(query.shape[-1])\n",
    "    scores = query @ key.T * scale\n",
    "    scores_max = scores.max(axis=1, keepdims=True)\n",
    "    exp_scores = np.exp(scores - scores_max)\n",
    "    attention_weights = exp_scores / exp_scores.sum(axis=1, keepdims=True)\n",
    "    output = attention_weights @ value\n",
    "    return output, attention_weights\n",
    "\n",
    "naive_output, attention_weights = naive_attention(query, key, value)\n",
    "print(\"Naive attention output shape:\", naive_output.shape)\n",
    "print(\"\\nAttention weights (the NxN matrix we want to avoid storing):\")\n",
    "print(attention_weights.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ttmpotnqqtg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth softmax: [0.0276 0.2037 0.0749 0.5536 0.0167 0.1235]\n",
      "After block [1. 3.]: max=3.00, sum_exp=1.1353\n",
      "After block [2. 4.]: max=4.00, sum_exp=1.5530\n",
      "After block [0.5 2.5]: max=4.00, sum_exp=1.8063\n",
      "\n",
      "Online softmax:       [0.0276 0.2037 0.0749 0.5536 0.0167 0.1235]\n",
      "Match: True\n"
     ]
    }
   ],
   "source": [
    "# DEMO: Online softmax on a single row of scores\n",
    "scores_row = np.array([1.0, 3.0, 2.0, 4.0, 0.5, 2.5])\n",
    "block_size = 2\n",
    "\n",
    "# Ground truth\n",
    "scores_max = scores_row.max()\n",
    "ground_truth = np.exp(scores_row - scores_max) / np.exp(scores_row - scores_max).sum()\n",
    "print(\"Ground truth softmax:\", ground_truth.round(4))\n",
    "\n",
    "# Online computation\n",
    "running_max = -np.inf\n",
    "running_sum_exp = 0.0\n",
    "\n",
    "for block_start in range(0, len(scores_row), block_size):\n",
    "    block_scores = scores_row[block_start : block_start + block_size]\n",
    "    block_max = block_scores.max()\n",
    "    block_sum_exp = np.exp(block_scores - block_max).sum()\n",
    "    \n",
    "    new_max = max(running_max, block_max)\n",
    "    old_contribution = np.exp(running_max - new_max) * running_sum_exp\n",
    "    new_contribution = np.exp(block_max - new_max) * block_sum_exp\n",
    "    new_sum_exp = old_contribution + new_contribution\n",
    "    \n",
    "    running_max = new_max\n",
    "    running_sum_exp = new_sum_exp\n",
    "    print(f\"After block {block_scores}: max={running_max:.2f}, sum_exp={running_sum_exp:.4f}\")\n",
    "\n",
    "online_softmax = np.exp(scores_row - running_max) / running_sum_exp\n",
    "print(\"\\nOnline softmax:      \", online_softmax.round(4))\n",
    "print(\"Match:\", np.allclose(ground_truth, online_softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "enuwi1f6k3o",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive output:\n",
      "[[-0.2606  0.1387  0.187   0.178 ]\n",
      " [-0.2657  0.1958  0.3021  0.1886]\n",
      " [-0.2045  0.1917 -0.1595 -0.1489]\n",
      " [ 0.129  -0.2446  0.2879  0.6396]\n",
      " [-0.2408  0.0981 -0.2086 -0.4503]\n",
      " [ 0.058   0.6045 -0.2326  0.2756]\n",
      " [-0.1434 -0.0073 -0.0186 -0.1872]\n",
      " [-0.1516 -0.0692  0.1787 -0.2666]]\n",
      "\n",
      "Flash output:\n",
      "[[-0.2606  0.1387  0.187   0.178 ]\n",
      " [-0.2657  0.1958  0.3021  0.1886]\n",
      " [-0.2045  0.1917 -0.1595 -0.1489]\n",
      " [ 0.129  -0.2446  0.2879  0.6396]\n",
      " [-0.2408  0.0981 -0.2086 -0.4503]\n",
      " [ 0.058   0.6045 -0.2326  0.2756]\n",
      " [-0.1434 -0.0073 -0.0186 -0.1872]\n",
      " [-0.1516 -0.0692  0.1787 -0.2666]]\n",
      "\n",
      "Max absolute error: 5.9604645e-08\n",
      "Results match: True\n"
     ]
    }
   ],
   "source": [
    "# FLASH ATTENTION\n",
    "def flash_attention(query, key, value, block_size_query=2, block_size_key=2):\n",
    "    sequence_length, head_dimension = query.shape\n",
    "    scale = 1.0 / math.sqrt(head_dimension)\n",
    "    \n",
    "    output = np.zeros((sequence_length, head_dimension), dtype=np.float64)\n",
    "    running_max = np.full(sequence_length, -np.inf, dtype=np.float64)\n",
    "    running_sum_exp = np.zeros(sequence_length, dtype=np.float64)\n",
    "    \n",
    "    for key_block_start in range(0, sequence_length, block_size_key):\n",
    "        key_block_end = min(key_block_start + block_size_key, sequence_length)\n",
    "        key_block = key[key_block_start:key_block_end].astype(np.float64)\n",
    "        value_block = value[key_block_start:key_block_end].astype(np.float64)\n",
    "        \n",
    "        for query_block_start in range(0, sequence_length, block_size_query):\n",
    "            query_block_end = min(query_block_start + block_size_query, sequence_length)\n",
    "            query_block = query[query_block_start:query_block_end].astype(np.float64)\n",
    "            \n",
    "            current_max = running_max[query_block_start:query_block_end]\n",
    "            current_sum_exp = running_sum_exp[query_block_start:query_block_end]\n",
    "            current_output = output[query_block_start:query_block_end]\n",
    "            \n",
    "            scores_block = (query_block @ key_block.T) * scale\n",
    "            \n",
    "            block_max = scores_block.max(axis=1)\n",
    "            block_exp_scores = np.exp(scores_block - block_max[:, None])\n",
    "            block_sum_exp = block_exp_scores.sum(axis=1)\n",
    "            block_weighted_values = block_exp_scores @ value_block\n",
    "            \n",
    "            new_max = np.maximum(current_max, block_max)\n",
    "            old_scale = np.exp(current_max - new_max)\n",
    "            new_scale = np.exp(block_max - new_max)\n",
    "            new_sum_exp = old_scale * current_sum_exp + new_scale * block_sum_exp\n",
    "            \n",
    "            new_output = (\n",
    "                old_scale[:, None] * current_sum_exp[:, None] * current_output +\n",
    "                new_scale[:, None] * block_weighted_values\n",
    "            ) / new_sum_exp[:, None]\n",
    "            \n",
    "            output[query_block_start:query_block_end] = new_output\n",
    "            running_max[query_block_start:query_block_end] = new_max\n",
    "            running_sum_exp[query_block_start:query_block_end] = new_sum_exp\n",
    "    \n",
    "    return output.astype(query.dtype)\n",
    "\n",
    "# Compare\n",
    "flash_output = flash_attention(query, key, value, block_size_query=2, block_size_key=2)\n",
    "\n",
    "print(\"Naive output:\")\n",
    "print(naive_output.round(4))\n",
    "print(\"\\nFlash output:\")\n",
    "print(flash_output.round(4))\n",
    "print(\"\\nMax absolute error:\", np.abs(naive_output - flash_output).max())\n",
    "print(\"Results match:\", np.allclose(naive_output, flash_output, rtol=1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4afj9id82jg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block size  8: max error = 3.13e-07\n",
      "Block size 16: max error = 3.13e-07\n",
      "Block size 32: max error = 3.13e-07\n",
      "Block size 64: max error = 3.13e-07\n"
     ]
    }
   ],
   "source": [
    "# Test with larger dimensions and various block sizes\n",
    "np.random.seed(123)\n",
    "large_sequence_length = 128\n",
    "large_head_dimension = 64\n",
    "\n",
    "large_query = np.random.randn(large_sequence_length, large_head_dimension).astype(np.float32)\n",
    "large_key = np.random.randn(large_sequence_length, large_head_dimension).astype(np.float32)\n",
    "large_value = np.random.randn(large_sequence_length, large_head_dimension).astype(np.float32)\n",
    "\n",
    "large_naive_output, _ = naive_attention(large_query, large_key, large_value)\n",
    "\n",
    "for block_size in [8, 16, 32, 64]:\n",
    "    large_flash_output = flash_attention(\n",
    "        large_query, large_key, large_value,\n",
    "        block_size_query=block_size, \n",
    "        block_size_key=block_size\n",
    "    )\n",
    "    max_error = np.abs(large_naive_output - large_flash_output).max()\n",
    "    print(f\"Block size {block_size:2d}: max error = {max_error:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ujd6arkcyl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All scores: [  1.   2. 100. 101.]\n",
      "\n",
      "=== NAIVE (broken) ===\n",
      "exp(block_1) = [2.71828183 7.3890561 ]\n",
      "exp(block_2) = [2.68811714e+43 7.30705998e+43]\n",
      "^ See the problem? exp(100) and exp(101) overflow to inf!\n",
      "\n",
      "=== STANDARD STABLE SOFTMAX (needs global max) ===\n",
      "Global max = 101.0\n",
      "exp(scores - global_max) = [3.72007598e-44 1.01122149e-43 3.67879441e-01 1.00000000e+00]\n",
      "sum = 1.3678794411714423\n",
      "softmax = [2.71959346e-44 7.39262147e-44 2.68941421e-01 7.31058579e-01]\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# WHY CAN'T WE JUST ACCUMULATE exp(scores) NAIVELY?\n",
    "# =============================================================================\n",
    "\n",
    "# The problem: numerical overflow/underflow\n",
    "\n",
    "# Imagine we have scores across two blocks:\n",
    "block_1_scores = np.array([1.0, 2.0])\n",
    "block_2_scores = np.array([100.0, 101.0])\n",
    "all_scores = np.concatenate([block_1_scores, block_2_scores])\n",
    "\n",
    "print(\"All scores:\", all_scores)\n",
    "print()\n",
    "\n",
    "# NAIVE APPROACH: just accumulate exp(scores)\n",
    "print(\"=== NAIVE (broken) ===\")\n",
    "print(f\"exp(block_1) = {np.exp(block_1_scores)}\")\n",
    "print(f\"exp(block_2) = {np.exp(block_2_scores)}\")  # OVERFLOW!\n",
    "print(\"^ See the problem? exp(100) and exp(101) overflow to inf!\")\n",
    "print()\n",
    "\n",
    "# This is why standard softmax subtracts the max first\n",
    "print(\"=== STANDARD STABLE SOFTMAX (needs global max) ===\")\n",
    "global_max = all_scores.max()\n",
    "print(f\"Global max = {global_max}\")\n",
    "stable_exp = np.exp(all_scores - global_max)\n",
    "print(f\"exp(scores - global_max) = {stable_exp}\")\n",
    "print(f\"sum = {stable_exp.sum()}\")\n",
    "print(f\"softmax = {stable_exp / stable_exp.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fp7q12st3y",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== USING LOCAL MAX PER BLOCK ===\n",
      "\n",
      "Block 1: scores=[1. 2.], max=2.0\n",
      "         exp(scores - max) = [0.36787944 1.        ]\n",
      "         sum_exp = 1.3679\n",
      "\n",
      "Block 2: scores=[100. 101.], max=101.0\n",
      "         exp(scores - max) = [0.36787944 1.        ]\n",
      "         sum_exp = 1.3679\n",
      "\n",
      "WRONG: block_1_sum + block_2_sum = 2.7358\n",
      "\n",
      "=== THE PROBLEM: DIFFERENT COORDINATE SYSTEMS ===\n",
      "block_1_sum = sum(exp(scores - 2))   <-- relative to max=2\n",
      "block_2_sum = sum(exp(scores - 101)) <-- relative to max=101\n",
      "These are in different 'units' - can't add directly!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# OK, SO USE LOCAL MAX... BUT CAN WE JUST ADD THE SUMS?\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== USING LOCAL MAX PER BLOCK ===\")\n",
    "print()\n",
    "\n",
    "# Block 1: use local max = 2\n",
    "block_1_max = block_1_scores.max()\n",
    "block_1_exp = np.exp(block_1_scores - block_1_max)\n",
    "block_1_sum = block_1_exp.sum()\n",
    "print(f\"Block 1: scores={block_1_scores}, max={block_1_max}\")\n",
    "print(f\"         exp(scores - max) = {block_1_exp}\")\n",
    "print(f\"         sum_exp = {block_1_sum:.4f}\")\n",
    "print()\n",
    "\n",
    "# Block 2: use local max = 101  \n",
    "block_2_max = block_2_scores.max()\n",
    "block_2_exp = np.exp(block_2_scores - block_2_max)\n",
    "block_2_sum = block_2_exp.sum()\n",
    "print(f\"Block 2: scores={block_2_scores}, max={block_2_max}\")\n",
    "print(f\"         exp(scores - max) = {block_2_exp}\")\n",
    "print(f\"         sum_exp = {block_2_sum:.4f}\")\n",
    "print()\n",
    "\n",
    "# WRONG: Can we just add them?\n",
    "naive_total = block_1_sum + block_2_sum\n",
    "print(f\"WRONG: block_1_sum + block_2_sum = {naive_total:.4f}\")\n",
    "print()\n",
    "\n",
    "# These sums are in DIFFERENT COORDINATE SYSTEMS!\n",
    "# block_1_sum is relative to max=2\n",
    "# block_2_sum is relative to max=101\n",
    "# You can't add apples and oranges!\n",
    "\n",
    "print(\"=== THE PROBLEM: DIFFERENT COORDINATE SYSTEMS ===\")\n",
    "print(f\"block_1_sum = sum(exp(scores - 2))   <-- relative to max=2\")\n",
    "print(f\"block_2_sum = sum(exp(scores - 101)) <-- relative to max=101\")\n",
    "print(\"These are in different 'units' - can't add directly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "g42heo8lyad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONVERTING TO COMMON COORDINATES ===\n",
      "\n",
      "Global max (our reference) = 101.0\n",
      "\n",
      "Block 1 correction: exp(2.0 - 101.0) = exp(-99) = 1.01e-43\n",
      "Block 1 sum corrected: 1.3679 * 1.01e-43 = 1.38e-43\n",
      "\n",
      "Block 2 correction: exp(101.0 - 101.0) = exp(0) = 1.0\n",
      "Block 2 sum corrected: 1.3679 * 1.0 = 1.3679\n",
      "\n",
      "CORRECT total sum_exp = 1.3679\n",
      "\n",
      "=== VERIFICATION ===\n",
      "Ground truth sum_exp = 1.3679\n",
      "Match: True\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# THE FIX: CONVERT TO A COMMON COORDINATE SYSTEM\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== CONVERTING TO COMMON COORDINATES ===\")\n",
    "print()\n",
    "\n",
    "# To add sums, we need them in the SAME coordinate system (same reference max)\n",
    "# Choose the global max = 101 as our reference\n",
    "\n",
    "global_max = max(block_1_max, block_2_max)\n",
    "print(f\"Global max (our reference) = {global_max}\")\n",
    "print()\n",
    "\n",
    "# Convert block_1_sum from \"relative to max=2\" to \"relative to max=101\"\n",
    "# \n",
    "# Originally: block_1_sum = exp(1-2) + exp(2-2) = exp(-1) + exp(0)\n",
    "# We want:    block_1_sum = exp(1-101) + exp(2-101) = exp(-100) + exp(-99)\n",
    "#\n",
    "# Notice: exp(1-101) = exp(1-2) * exp(2-101) = exp(1-2) * exp(-99)\n",
    "#         exp(2-101) = exp(2-2) * exp(2-101) = exp(2-2) * exp(-99)\n",
    "#\n",
    "# So: new_sum = old_sum * exp(old_max - new_max)\n",
    "\n",
    "correction_factor_1 = np.exp(block_1_max - global_max)  # exp(2 - 101) = exp(-99)\n",
    "block_1_sum_corrected = block_1_sum * correction_factor_1\n",
    "\n",
    "print(f\"Block 1 correction: exp({block_1_max} - {global_max}) = exp(-99) = {correction_factor_1:.2e}\")\n",
    "print(f\"Block 1 sum corrected: {block_1_sum:.4f} * {correction_factor_1:.2e} = {block_1_sum_corrected:.2e}\")\n",
    "print()\n",
    "\n",
    "# Block 2 is already in the right coordinate system (its max IS the global max)\n",
    "correction_factor_2 = np.exp(block_2_max - global_max)  # exp(101 - 101) = exp(0) = 1\n",
    "block_2_sum_corrected = block_2_sum * correction_factor_2\n",
    "\n",
    "print(f\"Block 2 correction: exp({block_2_max} - {global_max}) = exp(0) = {correction_factor_2}\")\n",
    "print(f\"Block 2 sum corrected: {block_2_sum:.4f} * {correction_factor_2} = {block_2_sum_corrected:.4f}\")\n",
    "print()\n",
    "\n",
    "# NOW we can add them!\n",
    "correct_total = block_1_sum_corrected + block_2_sum_corrected\n",
    "print(f\"CORRECT total sum_exp = {correct_total:.4f}\")\n",
    "print()\n",
    "\n",
    "# Verify against ground truth\n",
    "print(\"=== VERIFICATION ===\")\n",
    "ground_truth_sum = np.exp(all_scores - global_max).sum()\n",
    "print(f\"Ground truth sum_exp = {ground_truth_sum:.4f}\")\n",
    "print(f\"Match: {np.isclose(correct_total, ground_truth_sum)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45q59l894dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "THE KEY INSIGHT:\n",
      "================\n",
      "\n",
      "When you compute exp(score - local_max), you're working in a \"coordinate system\"\n",
      "centered at that local_max.\n",
      "\n",
      "   exp(score - max_A)  is in \"coordinate system A\"\n",
      "   exp(score - max_B)  is in \"coordinate system B\"\n",
      "\n",
      "You CANNOT add values from different coordinate systems!\n",
      "\n",
      "To convert from system A to system B:\n",
      "\n",
      "   value_in_B = value_in_A * exp(max_A - max_B)\n",
      "\n",
      "This is like converting currencies:\n",
      "   - Block 1 computed sums in \"max=2 dollars\"  \n",
      "   - Block 2 computed sums in \"max=101 dollars\"\n",
      "   - To add them, convert block 1 to \"max=101 dollars\" first\n",
      "   - The exchange rate is exp(2 - 101) = exp(-99) ≈ 0\n",
      "\n",
      "In this example, block 1's contribution becomes essentially ZERO because\n",
      "its scores (1, 2) are so much smaller than block 2's scores (100, 101).\n",
      "\n",
      "This is mathematically correct! In the final softmax:\n",
      "   - scores [1, 2] get probability ≈ 0  \n",
      "   - scores [100, 101] get probability ≈ [0.27, 0.73]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# THE INTUITION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\"\"\n",
    "THE KEY INSIGHT:\n",
    "================\n",
    "\n",
    "When you compute exp(score - local_max), you're working in a \"coordinate system\"\n",
    "centered at that local_max.\n",
    "\n",
    "   exp(score - max_A)  is in \"coordinate system A\"\n",
    "   exp(score - max_B)  is in \"coordinate system B\"\n",
    "\n",
    "You CANNOT add values from different coordinate systems!\n",
    "\n",
    "To convert from system A to system B:\n",
    "   \n",
    "   value_in_B = value_in_A * exp(max_A - max_B)\n",
    "\n",
    "This is like converting currencies:\n",
    "   - Block 1 computed sums in \"max=2 dollars\"  \n",
    "   - Block 2 computed sums in \"max=101 dollars\"\n",
    "   - To add them, convert block 1 to \"max=101 dollars\" first\n",
    "   - The exchange rate is exp(2 - 101) = exp(-99) ≈ 0\n",
    "\n",
    "In this example, block 1's contribution becomes essentially ZERO because\n",
    "its scores (1, 2) are so much smaller than block 2's scores (100, 101).\n",
    "\n",
    "This is mathematically correct! In the final softmax:\n",
    "   - scores [1, 2] get probability ≈ 0  \n",
    "   - scores [100, 101] get probability ≈ [0.27, 0.73]\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal-site",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
